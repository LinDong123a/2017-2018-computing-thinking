{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import struct\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from features import bulk_extract_features\n",
    "import go\n",
    "from sgf_wrapper import replay_sgf\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of data points to store in a chunk on disk\n",
    "CHUNK_SIZE = 4096\n",
    "CHUNK_HEADER_FORMAT = \"iii?\"\n",
    "CHUNK_HEADER_SIZE = struct.calcsize(CHUNK_HEADER_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def take_n(n, iterable):\n",
    "    return list(itertools.islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_chunks(chunk_size, iterator):\n",
    "    while True:\n",
    "        next_chunk = take_n(chunk_size, iterator)\n",
    "        # If len(iterable) % chunk_size == 0, don't return an empty chunk.\n",
    "        if next_chunk:\n",
    "            yield next_chunk\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_onehot(coords):\n",
    "    num_positions = len(coords)\n",
    "    output = np.zeros([num_positions, go.N ** 2], dtype=np.uint8)\n",
    "    for i, coord in enumerate(coords):\n",
    "        output[i, utils.flatten_coords(coord)] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_sgf_files(*dataset_dirs):\n",
    "    for dataset_dir in dataset_dirs:\n",
    "        full_dir = os.path.join(os.getcwd(), dataset_dir)\n",
    "        dataset_files = [os.path.join(full_dir, name) for name in os.listdir(full_dir)]\n",
    "        for f in dataset_files:\n",
    "            if os.path.isfile(f) and f.endswith(\".sgf\"):\n",
    "                yield f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_positions_from_sgf(file):\n",
    "    with open(file) as f:\n",
    "        for position_w_context in replay_sgf(f.read()):\n",
    "            if position_w_context.is_usable():\n",
    "                yield position_w_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_test_training(positions_w_context, est_num_positions):\n",
    "    print(\"Estimated number of chunks: %s\" % (est_num_positions // CHUNK_SIZE), file=sys.stderr)\n",
    "    desired_test_size = 10**5\n",
    "    if est_num_positions < 2 * desired_test_size:\n",
    "        positions_w_context = list(positions_w_context)\n",
    "        test_size = len(positions_w_context) // 3\n",
    "        return positions_w_context[:test_size], [positions_w_context[test_size:]]\n",
    "    else:\n",
    "        test_chunk = take_n(desired_test_size, positions_w_context)\n",
    "        training_chunks = iter_chunks(CHUNK_SIZE, positions_w_context)\n",
    "        return test_chunk, training_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    def __init__(self, pos_features, next_moves, results, is_test=False):\n",
    "        self.pos_features = pos_features\n",
    "        self.next_moves = next_moves\n",
    "        self.results = results\n",
    "        self.is_test = is_test\n",
    "        assert pos_features.shape[0] == next_moves.shape[0], \"Didn't pass in same number of pos_features and next_moves.\"\n",
    "        self.data_size = pos_features.shape[0]\n",
    "        self.board_size = pos_features.shape[1]\n",
    "        self.input_planes = pos_features.shape[-1]\n",
    "        self._index_within_epoch = 0\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        assert batch_size < self.data_size\n",
    "        if self._index_within_epoch + batch_size > self.data_size:\n",
    "            # Shuffle the data and start over\n",
    "            perm = np.arange(self.data_size)\n",
    "            np.random.shuffle(perm)\n",
    "            self.pos_features = self.pos_features[perm]\n",
    "            self.next_moves = self.next_moves[perm]\n",
    "            self._index_within_epoch = 0\n",
    "        start = self._index_within_epoch\n",
    "        end = start + batch_size\n",
    "        self._index_within_epoch += batch_size\n",
    "        return self.pos_features[start:end], self.next_moves[start:end]\n",
    "\n",
    "    @staticmethod\n",
    "    def from_positions_w_context(positions_w_context, is_test=False):\n",
    "        positions, next_moves, results = zip(*positions_w_context)\n",
    "        extracted_features = bulk_extract_features(positions)\n",
    "        encoded_moves = make_onehot(next_moves)\n",
    "        return DataSet(extracted_features, encoded_moves, results, is_test=is_test)\n",
    "\n",
    "    def write(self, filename):\n",
    "        header_bytes = struct.pack(CHUNK_HEADER_FORMAT, self.data_size, self.board_size, self.input_planes, self.is_test)\n",
    "        position_bytes = np.packbits(self.pos_features).tostring()\n",
    "        next_move_bytes = np.packbits(self.next_moves).tostring()\n",
    "        with gzip.open(filename, \"wb\", compresslevel=6) as f:\n",
    "            f.write(header_bytes)\n",
    "            f.write(position_bytes)\n",
    "            f.write(next_move_bytes)\n",
    "\n",
    "    @staticmethod\n",
    "    def read(filename):\n",
    "        with gzip.open(filename, \"rb\") as f:\n",
    "            header_bytes = f.read(CHUNK_HEADER_SIZE)\n",
    "            data_size, board_size, input_planes, is_test = struct.unpack(CHUNK_HEADER_FORMAT, header_bytes)\n",
    "\n",
    "            position_dims = data_size * board_size * board_size * input_planes\n",
    "            next_move_dims = data_size * board_size * board_size\n",
    "\n",
    "            # the +7 // 8 compensates for numpy's bitpacking padding\n",
    "            packed_position_bytes = f.read((position_dims + 7) // 8)\n",
    "            packed_next_move_bytes = f.read((next_move_dims + 7) // 8)\n",
    "            # should have cleanly finished reading all bytes from file!\n",
    "            assert len(f.read()) == 0\n",
    "\n",
    "            flat_position = np.unpackbits(np.fromstring(packed_position_bytes, dtype=np.uint8))[:position_dims]\n",
    "            flat_nextmoves = np.unpackbits(np.fromstring(packed_next_move_bytes, dtype=np.uint8))[:next_move_dims]\n",
    "\n",
    "            pos_features = flat_position.reshape(data_size, board_size, board_size, input_planes)\n",
    "            next_moves = flat_nextmoves.reshape(data_size, board_size * board_size)\n",
    "\n",
    "        return DataSet(pos_features, next_moves, [], is_test=is_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_data_sets(*data_sets):\n",
    "    sgf_files = list(find_sgf_files(*data_sets))\n",
    "    print(\"%s sgfs found.\" % len(sgf_files), file=sys.stderr)\n",
    "    est_num_positions = len(sgf_files) * 200 # about 200 moves per game\n",
    "    positions_w_context = itertools.chain(*map(get_positions_from_sgf, sgf_files))\n",
    "\n",
    "    test_chunk, training_chunks = split_test_training(positions_w_context, est_num_positions)\n",
    "    return test_chunk, training_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
