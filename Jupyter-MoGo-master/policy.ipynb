{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Neural network architecture.\n",
    "The input to the policy network is a 19 x 19 x 48 image stack consisting of\n",
    "48 feature planes. The first hidden layer zero pads the input into a 23 x 23\n",
    "image, then convolves k filters of kernel size 5 x 5 with stride 1 with the\n",
    "input image and applies a rectifier nonlinearity. Each of the subsequent\n",
    "hidden layers 2 to 12 zero pads the respective previous hidden layer into a\n",
    "21 x 21 image, then convolves k filters of kernel size 3 x 3 with stride 1,\n",
    "again followed by a rectifier nonlinearity. The final layer convolves 1 filter\n",
    "of kernel size 1 x 1 with stride 1, with a different bias for each position,\n",
    "and applies a softmax function. The match version of AlphaGo used k = 192\n",
    "filters; Fig. 2b and Extended Data Table 3 additionally show the results\n",
    "of training with k = 128, 256 and 384 filters.\n",
    "\n",
    "The input to the value network is also a 19 x 19 x 48 image stack, with an\n",
    "additional binary feature plane describing the current colour to play.\n",
    "Hidden layers 2 to 11 are identical to the policy network, hidden layer 12\n",
    "is an additional convolution layer, hidden layer 13 convolves 1 filter of\n",
    "kernel size 1 x 1 with stride 1, and hidden layer 14 is a fully connected\n",
    "linear layer with 256 rectifier units. The output layer is a fully connected\n",
    "linear layer with a single tanh unit.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import features\n",
    "import go\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPSILON = 1e-35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(object):\n",
    "    def __init__(self, features=features.DEFAULT_FEATURES, k=32, num_int_conv_layers=3, use_cpu=False):\n",
    "        self.num_input_planes = sum(f.planes for f in features)\n",
    "        self.features = features\n",
    "        self.k = k\n",
    "        self.num_int_conv_layers = num_int_conv_layers\n",
    "        self.test_summary_writer = None\n",
    "        self.training_summary_writer = None\n",
    "        self.test_stats = StatisticsCollector()\n",
    "        self.training_stats = StatisticsCollector()\n",
    "        self.session = tf.Session()\n",
    "        if use_cpu:\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                self.set_up_network()\n",
    "        else:\n",
    "            self.set_up_network()\n",
    "\n",
    "    def set_up_network(self):\n",
    "        # a global_step variable allows epoch counts to persist through multiple training sessions\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        x = tf.placeholder(tf.float32, [None, go.N, go.N, self.num_input_planes])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, go.N ** 2])\n",
    "\n",
    "        #convenience functions for initializing weights and biases\n",
    "        def _weight_variable(shape, name):\n",
    "            # If shape is [5, 5, 20, 32], then each of the 32 output planes\n",
    "            # has 5 * 5 * 20 inputs.\n",
    "            number_inputs_added = utils.product(shape[:-1])\n",
    "            stddev = 1 / math.sqrt(number_inputs_added)\n",
    "            # http://neuralnetworksanddeeplearning.com/chap3.html#weight_initialization\n",
    "            return tf.Variable(tf.truncated_normal(shape, stddev=stddev), name=name)\n",
    "\n",
    "        def _conv2d(x, W):\n",
    "            return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\")\n",
    "\n",
    "        # initial conv layer is 5x5\n",
    "        W_conv_init = _weight_variable([5, 5, self.num_input_planes, self.k], name=\"W_conv_init\")\n",
    "        h_conv_init = tf.nn.relu(_conv2d(x, W_conv_init), name=\"h_conv_init\")\n",
    "\n",
    "        # followed by a series of 3x3 conv layers\n",
    "        W_conv_intermediate = []\n",
    "        h_conv_intermediate = []\n",
    "        _current_h_conv = h_conv_init\n",
    "        for i in range(self.num_int_conv_layers):\n",
    "            with tf.name_scope(\"layer\"+str(i)):\n",
    "                W_conv_intermediate.append(_weight_variable([3, 3, self.k, self.k], name=\"W_conv\"))\n",
    "                h_conv_intermediate.append(tf.nn.relu(_conv2d(_current_h_conv, W_conv_intermediate[-1]), name=\"h_conv\"))\n",
    "                _current_h_conv = h_conv_intermediate[-1]\n",
    "\n",
    "        W_conv_final = _weight_variable([1, 1, self.k, 1], name=\"W_conv_final\")\n",
    "        b_conv_final = tf.Variable(tf.constant(0, shape=[go.N ** 2], dtype=tf.float32), name=\"b_conv_final\")\n",
    "        h_conv_final = _conv2d(h_conv_intermediate[-1], W_conv_final)\n",
    "\n",
    "        # Add epsilon to avoid taking the log of 0 in following step\n",
    "        output = tf.nn.softmax(tf.reshape(h_conv_final, [-1, go.N ** 2]) + b_conv_final) + tf.constant(EPSILON)\n",
    "\n",
    "        log_likelihood_cost = -tf.reduce_mean(tf.reduce_sum(tf.multiply(tf.log(output), y), reduction_indices=[1]))\n",
    "\n",
    "        # The step size was initialized to 0.003 and was halved every 80 million training steps\n",
    "        _learning_rate = tf.train.exponential_decay(3e-3, global_step,\n",
    "                                           8e7, 0.5)\n",
    "        train_step = tf.train.GradientDescentOptimizer(_learning_rate).minimize(log_likelihood_cost, global_step=global_step)\n",
    "        was_correct = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(was_correct, tf.float32))\n",
    "\n",
    "        weight_summaries = tf.summary.merge([\n",
    "            tf.summary.histogram(weight_var.name, weight_var)\n",
    "            for weight_var in [W_conv_init] +  W_conv_intermediate + [W_conv_final, b_conv_final]],\n",
    "            name=\"weight_summaries\"\n",
    "        )\n",
    "        activation_summaries = tf.summary.merge([\n",
    "            tf.summary.histogram(act_var.name, act_var)\n",
    "            for act_var in [h_conv_init] + h_conv_intermediate + [h_conv_final]],\n",
    "            name=\"activation_summaries\"\n",
    "        )\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # save everything to self.\n",
    "        for name, thing in locals().items():\n",
    "            if not name.startswith('_'):\n",
    "                setattr(self, name, thing)\n",
    "\n",
    "    def initialize_logging(self, tensorboard_logdir):\n",
    "        self.test_summary_writer = tf.summary.FileWriter(os.path.join(tensorboard_logdir, \"test\"), self.session.graph)\n",
    "        self.training_summary_writer = tf.summary.FileWriter(os.path.join(tensorboard_logdir, \"training\"), self.session.graph)\n",
    "\n",
    "    def initialize_variables(self, save_file=None):\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        if save_file is not None:\n",
    "            self.saver.restore(self.session, save_file)\n",
    "\n",
    "    def get_global_step(self):\n",
    "        return self.session.run(self.global_step)\n",
    "\n",
    "    def save_variables(self, save_file):\n",
    "        if save_file is not None:\n",
    "            print(\"Saving checkpoint to %s\" % save_file, file=sys.stderr)\n",
    "            self.saver.save(self.session, save_file)\n",
    "\n",
    "    def train(self, training_data, batch_size=32):\n",
    "        num_minibatches = training_data.data_size // batch_size\n",
    "        for i in range(num_minibatches):\n",
    "            batch_x, batch_y = training_data.get_batch(batch_size)\n",
    "            _, accuracy, cost = self.session.run(\n",
    "                [self.train_step, self.accuracy, self.log_likelihood_cost],\n",
    "                feed_dict={self.x: batch_x, self.y: batch_y})\n",
    "            self.training_stats.report(accuracy, cost)\n",
    "\n",
    "        avg_accuracy, avg_cost, accuracy_summaries = self.training_stats.collect()\n",
    "        global_step = self.get_global_step()\n",
    "        print(\"Step %d training data accuracy: %g; cost: %g\" % (global_step, avg_accuracy, avg_cost))\n",
    "        if self.training_summary_writer is not None:\n",
    "            activation_summaries = self.session.run(\n",
    "                self.activation_summaries,\n",
    "                feed_dict={self.x: batch_x, self.y: batch_y})\n",
    "            self.training_summary_writer.add_summary(activation_summaries, global_step)\n",
    "            self.training_summary_writer.add_summary(accuracy_summaries, global_step)\n",
    "\n",
    "\n",
    "    def run(self, position):\n",
    "        'Return a sorted list of (probability, move) tuples'\n",
    "        processed_position = features.extract_features(position, features=self.features)\n",
    "        probabilities = self.session.run(self.output, feed_dict={self.x: processed_position[None, :]})[0]\n",
    "        return probabilities.reshape([go.N, go.N])\n",
    "\n",
    "    def check_accuracy(self, test_data, batch_size=128):\n",
    "        num_minibatches = test_data.data_size // batch_size\n",
    "        weight_summaries = self.session.run(self.weight_summaries)\n",
    "\n",
    "        for i in range(num_minibatches):\n",
    "            batch_x, batch_y = test_data.get_batch(batch_size)\n",
    "            accuracy, cost = self.session.run(\n",
    "                [self.accuracy, self.log_likelihood_cost],\n",
    "                feed_dict={self.x: batch_x, self.y: batch_y})\n",
    "            self.test_stats.report(accuracy, cost)\n",
    "\n",
    "        avg_accuracy, avg_cost, accuracy_summaries = self.test_stats.collect()\n",
    "        global_step = self.get_global_step()\n",
    "        print(\"Step %s test data accuracy: %g; cost: %g\" % (global_step, avg_accuracy, avg_cost))\n",
    "\n",
    "        if self.test_summary_writer is not None:\n",
    "            self.test_summary_writer.add_summary(weight_summaries, global_step)\n",
    "            self.test_summary_writer.add_summary(accuracy_summaries, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StatisticsCollector(object):\n",
    "    '''\n",
    "    Accuracy and cost cannot be calculated with the full test dataset\n",
    "    in one pass, so they must be computed in batches. Unfortunately,\n",
    "    the built-in TF summary nodes cannot be told to aggregate multiple\n",
    "    executions. Therefore, we aggregate the accuracy/cost ourselves at\n",
    "    the python level, and then shove it through the accuracy/cost summary\n",
    "    nodes to generate the appropriate summary protobufs for writing.\n",
    "    '''\n",
    "    graph = tf.Graph()\n",
    "    with tf.device(\"/cpu:0\"), graph.as_default():\n",
    "        accuracy = tf.placeholder(tf.float32, [])\n",
    "        cost = tf.placeholder(tf.float32, [])\n",
    "        accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        cost_summary = tf.summary.scalar(\"log_likelihood_cost\", cost)\n",
    "        accuracy_summaries = tf.summary.merge([accuracy_summary, cost_summary], name=\"accuracy_summaries\")\n",
    "    session = tf.Session(graph=graph)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.accuracies = []\n",
    "        self.costs = []\n",
    "\n",
    "    def report(self, accuracy, cost):\n",
    "        self.accuracies.append(accuracy)\n",
    "        self.costs.append(cost)\n",
    "\n",
    "    def collect(self):\n",
    "        avg_acc = sum(self.accuracies) / len(self.accuracies)\n",
    "        avg_cost = sum(self.costs) / len(self.costs)\n",
    "        self.accuracies = []\n",
    "        self.costs = []\n",
    "        summary = self.session.run(self.accuracy_summaries,\n",
    "            feed_dict={self.accuracy:avg_acc, self.cost: avg_cost})\n",
    "        return avg_acc, avg_cost, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
